# Evaluation Runs

Execute and manage evaluation runs to assess the performance of AI models and functions.

## Overview

Evaluation Runs provide a way to execute and manage evaluations of AI models and functions. Evaluation Runs can:

- Execute evaluations on models and functions
- Track evaluation progress and status
- Collect evaluation results
- Compare performance across different models and functions

## Key Features

- **Evaluation Execution**: Execute evaluations on models and functions
- **Progress Tracking**: Track evaluation progress and status
- **Result Collection**: Collect evaluation results
- **Performance Comparison**: Compare performance across different models and functions

## Creating Evaluation Runs

Create evaluation runs using the Evals.do API:

```typescript
// Create an evaluation run
const run = await evals.runs.create({
  name: 'Text Classification Evaluation',
  description: 'Evaluating text classification models',
  eval: 'eval-123', // Reference to the evaluation configuration
  target: {
    type: 'function',
    id: 'func-123'
  },
  dataset: 'ds-123',
  parameters: {
    batchSize: 32,
    maxExamples: 1000
  }
})
```

## Managing Evaluation Runs

Manage evaluation runs using the Evals.do API:

```typescript
// Get all evaluation runs
const runs = await evals.runs.list({
  limit: 10,
  offset: 0
})

// Get a specific evaluation run
const run = await evals.runs.get('run-123')

// Update an evaluation run
const updatedRun = await evals.runs.update('run-123', {
  name: 'Updated Text Classification Evaluation',
  description: 'Updated evaluation of text classification models'
})

// Delete an evaluation run
await evals.runs.delete('run-123')
```

## Controlling Evaluation Runs

Control evaluation runs using the Evals.do API:

```typescript
// Start an evaluation run
await evals.runs.start('run-123')

// Pause an evaluation run
await evals.runs.pause('run-123')

// Resume an evaluation run
await evals.runs.resume('run-123')

// Cancel an evaluation run
await evals.runs.cancel('run-123')

// Restart an evaluation run
await evals.runs.restart('run-123')
```

## Evaluation Run Status

Get evaluation run status using the Evals.do API:

```typescript
// Get evaluation run status
const status = await evals.runs.getStatus('run-123')

// Get evaluation run progress
const progress = await evals.runs.getProgress('run-123')

// Get evaluation run logs
const logs = await evals.runs.getLogs('run-123', {
  limit: 10,
  offset: 0
})
```

## Evaluation Run Results

Get evaluation run results using the Evals.do API:

```typescript
// Get evaluation run results
const results = await evals.runs.getResults('run-123')

// Get evaluation run metrics
const metrics = await evals.runs.getMetrics('run-123')

// Get evaluation run examples
const examples = await evals.runs.getExamples('run-123', {
  limit: 10,
  offset: 0
})

// Get a specific example result
const example = await evals.runs.getExample('run-123', 'ex-123')
```

## Evaluation Run Comparison

Compare evaluation runs using the Evals.do API:

```typescript
// Compare evaluation runs
const comparison = await evals.runs.compare(['run-123', 'run-456'])

// Compare evaluation run metrics
const metricComparison = await evals.runs.compareMetrics(['run-123', 'run-456'], {
  metrics: ['accuracy', 'f1_score', 'precision', 'recall']
})

// Compare evaluation run examples
const exampleComparison = await evals.runs.compareExamples(['run-123', 'run-456'], {
  limit: 10,
  offset: 0
})
```

## Evaluation Run Configuration

Configure evaluation runs using the Evals.do API:

```typescript
// Create an evaluation run configuration
const config = await evals.runs.createConfig({
  name: 'Text Classification Config',
  description: 'Configuration for text classification evaluations',
  parameters: {
    batchSize: 32,
    maxExamples: 1000,
    metrics: ['accuracy', 'f1_score', 'precision', 'recall']
  }
})

// Get all evaluation run configurations
const configs = await evals.runs.listConfigs({
  limit: 10,
  offset: 0
})

// Get a specific evaluation run configuration
const config = await evals.runs.getConfig('config-123')

// Update an evaluation run configuration
const updatedConfig = await evals.runs.updateConfig('config-123', {
  name: 'Updated Text Classification Config',
  description: 'Updated configuration for text classification evaluations',
  parameters: {
    batchSize: 64,
    maxExamples: 2000,
    metrics: ['accuracy', 'f1_score', 'precision', 'recall', 'roc_auc']
  }
})

// Delete an evaluation run configuration
await evals.runs.deleteConfig('config-123')

// Create an evaluation run from a configuration
const run = await evals.runs.createFromConfig({
  config: 'config-123',
  target: {
    type: 'function',
    id: 'func-123'
  },
  dataset: 'ds-123'
})
```

## Evaluation Run Scheduling

Schedule evaluation runs using the Evals.do API:

```typescript
// Schedule an evaluation run
const schedule = await evals.runs.schedule({
  name: 'Scheduled Text Classification Evaluation',
  description: 'Scheduled evaluation of text classification models',
  eval: 'eval-123',
  target: {
    type: 'function',
    id: 'func-123'
  },
  dataset: 'ds-123',
  schedule: {
    frequency: 'daily',
    time: '00:00:00Z',
    startDate: '2023-07-01',
    endDate: '2023-12-31'
  }
})

// Get all scheduled evaluation runs
const schedules = await evals.runs.listSchedules({
  limit: 10,
  offset: 0
})

// Get a specific scheduled evaluation run
const schedule = await evals.runs.getSchedule('sched-123')

// Update a scheduled evaluation run
const updatedSchedule = await evals.runs.updateSchedule('sched-123', {
  name: 'Updated Scheduled Text Classification Evaluation',
  description: 'Updated scheduled evaluation of text classification models',
  schedule: {
    frequency: 'weekly',
    dayOfWeek: 'monday',
    time: '00:00:00Z'
  }
})

// Delete a scheduled evaluation run
await evals.runs.deleteSchedule('sched-123')

// Pause a scheduled evaluation run
await evals.runs.pauseSchedule('sched-123')

// Resume a scheduled evaluation run
await evals.runs.resumeSchedule('sched-123')
```

## Evaluation Run Notifications

Configure evaluation run notifications using the Evals.do API:

```typescript
// Configure evaluation run notifications
await evals.runs.configureNotifications('run-123', {
  onStart: {
    email: ['user1@example.com', 'user2@example.com'],
    slack: ['#evaluations']
  },
  onComplete: {
    email: ['user1@example.com', 'user2@example.com', 'user3@example.com'],
    slack: ['#evaluations', '#results']
  },
  onError: {
    email: ['user1@example.com', 'admin@example.com'],
    slack: ['#evaluations', '#alerts']
  }
})

// Get evaluation run notification configuration
const notifications = await evals.runs.getNotifications('run-123')

// Update evaluation run notification configuration
await evals.runs.updateNotifications('run-123', {
  onComplete: {
    email: ['user1@example.com', 'user2@example.com', 'user4@example.com'],
    slack: ['#evaluations', '#results']
  }
})

// Delete evaluation run notification configuration
await evals.runs.deleteNotifications('run-123')
```

## Evaluation Run Exports

Export evaluation run results using the Evals.do API:

```typescript
// Export evaluation run results
const exportUrl = await evals.runs.export('run-123', {
  format: 'json',
  includeExamples: true,
  includeMetrics: true,
  includeLogs: false
})

// Export evaluation run comparison
const comparisonExportUrl = await evals.runs.exportComparison(['run-123', 'run-456'], {
  format: 'csv',
  metrics: ['accuracy', 'f1_score', 'precision', 'recall']
})

// Generate an evaluation run report
const reportUrl = await evals.runs.generateReport('run-123', {
  format: 'pdf',
  includeExamples: true,
  includeMetrics: true,
  includeLogs: false,
  includeCharts: true
})

// Generate an evaluation run comparison report
const comparisonReportUrl = await evals.runs.generateComparisonReport(['run-123', 'run-456'], {
  format: 'pdf',
  metrics: ['accuracy', 'f1_score', 'precision', 'recall'],
  includeExamples: true,
  includeCharts: true
})
```

## Evaluation Run Sharing

Share evaluation runs using the Evals.do API:

```typescript
// Share an evaluation run with a user
await evals.runs.share('run-123', {
  user: 'user-123',
  permission: 'read'
})

// Share an evaluation run with a team
await evals.runs.share('run-123', {
  team: 'team-123',
  permission: 'write'
})

// Share an evaluation run with an organization
await evals.runs.share('run-123', {
  organization: 'org-123',
  permission: 'admin'
})

// Get evaluation run permissions
const permissions = await evals.runs.getPermissions('run-123')

// Update evaluation run permissions
await evals.runs.updatePermissions('run-123', {
  user: 'user-123',
  permission: 'write'
})

// Remove evaluation run permissions
await evals.runs.removePermissions('run-123', {
  user: 'user-123'
})
```

## Next Steps

- [Create your first evaluation run](/evals/runs/create)
- [Configure evaluation parameters](/evals/runs/configuration)
- [Analyze evaluation results](/evals/runs/analysis)