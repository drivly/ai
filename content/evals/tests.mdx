# Tests

Tests verify that your AI applications behave as expected and meet quality standards.

## Overview

The Tests collection provides a way to define, run, and analyze tests for your AI applications. Tests can:

- Verify functionality and behavior
- Ensure quality and correctness
- Catch regressions and issues
- Guide development and improvement

## Key Features

- **Verification**: Verify that your applications behave as expected
- **Automation**: Automate testing processes
- **Reporting**: Generate test reports and insights
- **Integration**: Integrate with CI/CD pipelines

## Defining Tests

Tests can be defined using the Evals.do API or through the dashboard interface.

```typescript
// Example test definition
const ResponseGenerationTest = {
  name: 'response_generation_test',
  description: 'Test response generation functionality',
  type: 'unit',
  function: 'generateResponse',
  cases: [
    {
      id: 'simple_question',
      input: {
        prompt: 'What is the capital of France?',
        options: {
          temperature: 0.0,
          maxTokens: 100
        }
      },
      assertions: [
        {
          type: 'contains',
          value: 'Paris',
          message: 'Response should mention Paris'
        },
        {
          type: 'not_contains',
          value: 'London',
          message: 'Response should not mention London'
        },
        {
          type: 'max_tokens',
          value: 50,
          message: 'Response should be concise'
        }
      ]
    },
    {
      id: 'complex_question',
      input: {
        prompt: 'Explain the process of photosynthesis.',
        options: {
          temperature: 0.0,
          maxTokens: 300
        }
      },
      assertions: [
        {
          type: 'contains_all',
          value: ['chlorophyll', 'sunlight', 'carbon dioxide', 'oxygen'],
          message: 'Response should mention key photosynthesis concepts'
        },
        {
          type: 'min_tokens',
          value: 100,
          message: 'Response should be detailed enough'
        }
      ]
    }
  ],
  setup: async () => {
    // Setup code...
  },
  teardown: async () => {
    // Teardown code...
  }
}
```

## Test Categories

Evals.do supports various test categories:

### Unit Tests

Test individual functions and components:

```typescript
const TextProcessingTest = {
  name: 'text_processing_test',
  type: 'unit',
  function: 'processText',
  cases: [
    {
      id: 'empty_text',
      input: { text: '' },
      assertions: [
        {
          type: 'equals',
          value: '',
          message: 'Empty text should return empty result'
        }
      ]
    },
    // More test cases...
  ]
}
```

### Integration Tests

Test interactions between components:

```typescript
const WorkflowIntegrationTest = {
  name: 'workflow_integration_test',
  type: 'integration',
  workflow: 'content_generation_workflow',
  cases: [
    {
      id: 'generate_article',
      input: {
        topic: 'Artificial Intelligence',
        length: 'medium',
        style: 'informative'
      },
      assertions: [
        {
          type: 'workflow_success',
          message: 'Workflow should complete successfully'
        },
        {
          type: 'step_success',
          value: 'generate_outline',
          message: 'Outline generation step should succeed'
        },
        {
          type: 'output_contains',
          path: 'article',
          value: 'Artificial Intelligence',
          message: 'Article should mention the topic'
        }
      ]
    }
  ]
}
```

### End-to-End Tests

Test complete user flows:

```typescript
const UserOnboardingTest = {
  name: 'user_onboarding_test',
  type: 'e2e',
  flow: 'user_onboarding',
  cases: [
    {
      id: 'new_user_signup',
      steps: [
        {
          action: 'createAccount',
          input: {
            email: 'test@example.com',
            password: 'Test123!',
            name: 'Test User'
          },
          assertions: [
            {
              type: 'success',
              message: 'Account creation should succeed'
            }
          ]
        },
        {
          action: 'verifyEmail',
          assertions: [
            {
              type: 'success',
              message: 'Email verification should succeed'
            }
          ]
        },
        {
          action: 'completeProfile',
          input: {
            bio: 'Test bio',
            interests: ['AI', 'Technology']
          },
          assertions: [
            {
              type: 'success',
              message: 'Profile completion should succeed'
            }
          ]
        }
      ]
    }
  ]
}
```

### Performance Tests

Test application performance:

```typescript
const APIPerformanceTest = {
  name: 'api_performance_test',
  type: 'performance',
  endpoint: '/api/generate',
  method: 'POST',
  payload: {
    prompt: 'Generate a short story about a robot.',
    options: {
      temperature: 0.7,
      maxTokens: 500
    }
  },
  scenarios: [
    {
      name: 'light_load',
      users: 10,
      duration: '1m',
      rampUp: '10s',
      assertions: [
        {
          type: 'p95_response_time',
          value: 1000,
          message: 'P95 response time should be under 1000ms'
        },
        {
          type: 'error_rate',
          value: 0.01,
          message: 'Error rate should be under 1%'
        }
      ]
    },
    {
      name: 'heavy_load',
      users: 50,
      duration: '2m',
      rampUp: '30s',
      assertions: [
        {
          type: 'p95_response_time',
          value: 2000,
          message: 'P95 response time should be under 2000ms'
        },
        {
          type: 'error_rate',
          value: 0.05,
          message: 'Error rate should be under 5%'
        }
      ]
    }
  ]
}
```

## Running Tests

Tests can be run using the Evals.do API:

```typescript
// Run a single test
const testResult = await tests.run('response_generation_test')

// Run multiple tests
const testResults = await tests.runBatch([
  'response_generation_test',
  'text_processing_test'
])

// Run tests with specific options
const testResultWithOptions = await tests.run('response_generation_test', {
  cases: ['simple_question'],
  environment: 'staging',
  timeout: 30000
})

// Run tests in CI/CD pipeline
const ciTestResults = await tests.runForCI({
  tests: ['response_generation_test', 'workflow_integration_test'],
  environment: 'ci',
  reportFormat: 'junit'
})
```

## Analyzing Test Results

Analyze test results using the Evals.do API:

```typescript
// Get test results
const results = await tests.getResults('response_generation_test', {
  timeRange: {
    start: '2023-06-01T00:00:00Z',
    end: '2023-06-30T23:59:59Z'
  }
})

// Get test trends
const trends = await tests.getTrends('response_generation_test', {
  timeRange: {
    start: '2023-01-01T00:00:00Z',
    end: '2023-06-30T23:59:59Z'
  },
  groupBy: 'week'
})

// Compare test results
const comparison = await tests.compareResults('response_generation_test', {
  baseline: {
    runId: 'run_123',
    label: 'Before changes'
  },
  current: {
    runId: 'run_456',
    label: 'After changes'
  }
})
```

## Test Management

Manage your tests through the dashboard or API:

```typescript
// Create a new test
await tests.create({
  name: 'new_test',
  // Test definition...
})

// Update a test
await tests.update('new_test', {
  // Updated test definition...
})

// Delete a test
await tests.delete('new_test')
```

## Next Steps

- [Create your first test](/evals/tests/create)
- [Learn about test assertions](/evals/tests/assertions)
- [Explore test templates](/evals/tests/templates)