# Metrics

Metrics provide quantitative measurements of your AI applications' performance and behavior.

## Overview

The Metrics collection provides a way to define, collect, and analyze metrics for your AI applications. Metrics can:

- Measure performance and quality
- Track usage and costs
- Identify issues and bottlenecks
- Guide optimization efforts

## Key Features

- **Measurement**: Collect quantitative data about your applications
- **Aggregation**: Combine and summarize metrics
- **Visualization**: View metrics in charts and dashboards
- **Alerting**: Set up alerts based on metric thresholds

## Defining Metrics

Metrics can be defined using the Evals.do API or through the dashboard interface.

```typescript
// Example metric definition
const ResponseTimeMetric = {
  name: 'response_time',
  description: 'Time taken to generate a response',
  unit: 'milliseconds',
  type: 'histogram',
  labels: ['model', 'function', 'environment'],
  aggregations: [
    { type: 'avg', window: '1m' },
    { type: 'p95', window: '1m' },
    { type: 'p99', window: '1m' },
    { type: 'avg', window: '1h' },
    { type: 'p95', window: '1h' },
    { type: 'p99', window: '1h' },
  ],
  thresholds: [
    { level: 'warning', value: 1000, aggregation: 'avg', window: '1m' },
    { level: 'critical', value: 2000, aggregation: 'avg', window: '1m' },
  ],
}
```

## Metric Categories

Evals.do supports various metric categories:

### Performance Metrics

Measure application performance:

```typescript
const LatencyMetric = {
  name: 'latency',
  description: 'End-to-end latency of requests',
  unit: 'milliseconds',
  type: 'histogram',
  // Metric definition...
}

const ThroughputMetric = {
  name: 'throughput',
  description: 'Number of requests processed per second',
  unit: 'requests/second',
  type: 'gauge',
  // Metric definition...
}
```

### Quality Metrics

Measure output quality:

```typescript
const AccuracyMetric = {
  name: 'accuracy',
  description: 'Accuracy of model predictions',
  unit: 'percentage',
  type: 'gauge',
  // Metric definition...
}

const RelevanceMetric = {
  name: 'relevance',
  description: 'Relevance of generated content',
  unit: 'score',
  type: 'gauge',
  range: [0, 10],
  // Metric definition...
}
```

### Usage Metrics

Measure application usage:

```typescript
const RequestCountMetric = {
  name: 'request_count',
  description: 'Number of requests',
  unit: 'count',
  type: 'counter',
  // Metric definition...
}

const TokenUsageMetric = {
  name: 'token_usage',
  description: 'Number of tokens used',
  unit: 'tokens',
  type: 'counter',
  labels: ['model', 'function', 'type'],
  // Metric definition...
}
```

### Cost Metrics

Measure application costs:

```typescript
const APICallCostMetric = {
  name: 'api_call_cost',
  description: 'Cost of API calls',
  unit: 'USD',
  type: 'counter',
  labels: ['model', 'function'],
  // Metric definition...
}
```

## Collecting Metrics

Metrics can be collected using the Evals.do API:

```typescript
// Record a metric value
await metrics.record('response_time', 150, {
  labels: {
    model: 'gpt-4',
    function: 'generateResponse',
    environment: 'production',
  },
})

// Record multiple metric values
await metrics.recordBatch([
  {
    name: 'response_time',
    value: 150,
    labels: {
      model: 'gpt-4',
      function: 'generateResponse',
      environment: 'production',
    },
  },
  {
    name: 'token_usage',
    value: 250,
    labels: {
      model: 'gpt-4',
      function: 'generateResponse',
      type: 'prompt',
    },
  },
  {
    name: 'token_usage',
    value: 100,
    labels: {
      model: 'gpt-4',
      function: 'generateResponse',
      type: 'completion',
    },
  },
])

// Use metric middleware
const functionWithMetrics = metrics.withMetrics(generateResponse, {
  metrics: ['response_time', 'token_usage', 'api_call_cost'],
  labels: {
    function: 'generateResponse',
    model: 'gpt-4',
  },
})
```

## Querying Metrics

Query metrics using the Evals.do API:

```typescript
// Query a metric
const responseTimeData = await metrics.query('response_time', {
  timeRange: {
    start: '2023-06-01T00:00:00Z',
    end: '2023-06-30T23:59:59Z',
  },
  aggregation: 'avg',
  window: '1h',
  filters: {
    model: 'gpt-4',
    environment: 'production',
  },
  groupBy: ['function'],
})

// Compare metrics
const comparisonData = await metrics.compare('response_time', {
  timeRanges: [
    {
      start: '2023-05-01T00:00:00Z',
      end: '2023-05-31T23:59:59Z',
      label: 'May',
    },
    {
      start: '2023-06-01T00:00:00Z',
      end: '2023-06-30T23:59:59Z',
      label: 'June',
    },
  ],
  aggregation: 'avg',
  window: '1d',
  filters: {
    model: 'gpt-4',
    environment: 'production',
  },
})
```

## Metric Management

Manage your metrics through the dashboard or API:

```typescript
// Create a new metric
await metrics.create({
  name: 'new_metric',
  // Metric definition...
})

// Update a metric
await metrics.update('new_metric', {
  // Updated metric definition...
})

// Delete a metric
await metrics.delete('new_metric')
```

## Next Steps

- [Create your first metric](/evals/metrics/create)
- [Learn about metric visualization](/evals/metrics/visualization)
- [Explore metric templates](/evals/metrics/templates)
