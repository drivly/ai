# Evaluation Results

Analyze and interpret the results of AI model and function evaluations.

## Overview

Evaluation Results provide a way to analyze and interpret the outcomes of AI model and function evaluations. Evaluation Results can:

- Provide performance metrics
- Show detailed example-level results
- Enable result filtering and aggregation
- Support visualization and reporting

## Key Features

- **Performance Metrics**: View and analyze performance metrics
- **Example-Level Results**: Examine detailed results for individual examples
- **Result Filtering**: Filter results based on various criteria
- **Result Aggregation**: Aggregate results across different dimensions
- **Visualization**: Visualize results with charts and graphs

## Accessing Evaluation Results

Access evaluation results using the Evals.do API:

```typescript
// Get all evaluation results
const results = await evals.results.list({
  limit: 10,
  offset: 0
})

// Get results for a specific evaluation run
const runResults = await evals.results.getByRun('run-123')

// Get a specific result
const result = await evals.results.get('result-123')

// Get results for a specific target (function, workflow, or agent)
const targetResults = await evals.results.getByTarget({
  type: 'function',
  id: 'func-123'
})

// Get results for a specific dataset
const datasetResults = await evals.results.getByDataset('ds-123')

// Get results for a specific evaluation
const evalResults = await evals.results.getByEval('eval-123')
```

## Result Metrics

Access result metrics using the Evals.do API:

```typescript
// Get metrics for a specific result
const metrics = await evals.results.getMetrics('result-123')

// Get metrics for a specific evaluation run
const runMetrics = await evals.results.getRunMetrics('run-123')

// Get metrics for a specific target
const targetMetrics = await evals.results.getTargetMetrics({
  type: 'function',
  id: 'func-123'
})

// Get metrics for a specific dataset
const datasetMetrics = await evals.results.getDatasetMetrics('ds-123')

// Get metrics for a specific evaluation
const evalMetrics = await evals.results.getEvalMetrics('eval-123')

// Get metrics over time
const metricsOverTime = await evals.results.getMetricsOverTime({
  target: {
    type: 'function',
    id: 'func-123'
  },
  metrics: ['accuracy', 'f1_score', 'precision', 'recall'],
  timeRange: {
    start: '2023-01-01T00:00:00Z',
    end: '2023-06-30T23:59:59Z'
  },
  interval: 'month'
})
```

## Example-Level Results

Access example-level results using the Evals.do API:

```typescript
// Get examples for a specific result
const examples = await evals.results.getExamples('result-123', {
  limit: 10,
  offset: 0
})

// Get examples for a specific evaluation run
const runExamples = await evals.results.getRunExamples('run-123', {
  limit: 10,
  offset: 0
})

// Get a specific example result
const example = await evals.results.getExample('result-123', 'ex-123')

// Get examples with errors
const errorExamples = await evals.results.getErrorExamples('result-123', {
  limit: 10,
  offset: 0
})

// Get examples with low scores
const lowScoreExamples = await evals.results.getLowScoreExamples('result-123', {
  metric: 'accuracy',
  threshold: 0.5,
  limit: 10,
  offset: 0
})

// Get examples with high scores
const highScoreExamples = await evals.results.getHighScoreExamples('result-123', {
  metric: 'accuracy',
  threshold: 0.9,
  limit: 10,
  offset: 0
})
```

## Result Filtering

Filter results using the Evals.do API:

```typescript
// Filter results by criteria
const filteredResults = await evals.results.filter({
  runs: ['run-123', 'run-456'],
  targets: [
    {
      type: 'function',
      id: 'func-123'
    },
    {
      type: 'function',
      id: 'func-456'
    }
  ],
  datasets: ['ds-123', 'ds-456'],
  evals: ['eval-123', 'eval-456'],
  metrics: {
    accuracy: {
      min: 0.8,
      max: 1.0
    },
    f1_score: {
      min: 0.7
    }
  },
  timeRange: {
    start: '2023-01-01T00:00:00Z',
    end: '2023-06-30T23:59:59Z'
  },
  limit: 10,
  offset: 0
})

// Filter examples by criteria
const filteredExamples = await evals.results.filterExamples('result-123', {
  metrics: {
    accuracy: {
      min: 0.8,
      max: 1.0
    }
  },
  input: {
    text: {
      contains: 'product'
    }
  },
  output: {
    category: ['positive', 'negative']
  },
  prediction: {
    category: ['positive']
  },
  limit: 10,
  offset: 0
})
```

## Result Aggregation

Aggregate results using the Evals.do API:

```typescript
// Aggregate results by dimension
const aggregatedResults = await evals.results.aggregate({
  runs: ['run-123', 'run-456'],
  groupBy: ['target', 'dataset'],
  metrics: ['accuracy', 'f1_score', 'precision', 'recall'],
  timeRange: {
    start: '2023-01-01T00:00:00Z',
    end: '2023-06-30T23:59:59Z'
  }
})

// Aggregate examples by dimension
const aggregatedExamples = await evals.results.aggregateExamples('result-123', {
  groupBy: ['output.category', 'prediction.category'],
  metrics: ['accuracy', 'f1_score']
})

// Get confusion matrix
const confusionMatrix = await evals.results.getConfusionMatrix('result-123', {
  actual: 'output.category',
  predicted: 'prediction.category'
})

// Get distribution of predictions
const predictionDistribution = await evals.results.getPredictionDistribution('result-123', {
  field: 'prediction.category'
})

// Get distribution of errors
const errorDistribution = await evals.results.getErrorDistribution('result-123', {
  groupBy: 'output.category'
})
```

## Result Comparison

Compare results using the Evals.do API:

```typescript
// Compare results
const comparison = await evals.results.compare(['result-123', 'result-456'])

// Compare metrics
const metricComparison = await evals.results.compareMetrics(['result-123', 'result-456'], {
  metrics: ['accuracy', 'f1_score', 'precision', 'recall']
})

// Compare examples
const exampleComparison = await evals.results.compareExamples(['result-123', 'result-456'], {
  limit: 10,
  offset: 0
})

// Compare targets
const targetComparison = await evals.results.compareTargets([
  {
    type: 'function',
    id: 'func-123'
  },
  {
    type: 'function',
    id: 'func-456'
  }
], {
  metrics: ['accuracy', 'f1_score', 'precision', 'recall'],
  dataset: 'ds-123'
})

// Compare datasets
const datasetComparison = await evals.results.compareDatasets(['ds-123', 'ds-456'], {
  metrics: ['accuracy', 'f1_score', 'precision', 'recall'],
  target: {
    type: 'function',
    id: 'func-123'
  }
})

// Compare evaluations
const evalComparison = await evals.results.compareEvals(['eval-123', 'eval-456'], {
  metrics: ['accuracy', 'f1_score', 'precision', 'recall'],
  target: {
    type: 'function',
    id: 'func-123'
  },
  dataset: 'ds-123'
})
```

## Result Visualization

Visualize results using the Evals.do API:

```typescript
// Generate a metric chart
const metricChart = await evals.results.generateMetricChart('result-123', {
  metric: 'accuracy',
  type: 'bar',
  groupBy: 'output.category'
})

// Generate a comparison chart
const comparisonChart = await evals.results.generateComparisonChart(['result-123', 'result-456'], {
  metrics: ['accuracy', 'f1_score', 'precision', 'recall'],
  type: 'radar'
})

// Generate a confusion matrix chart
const confusionMatrixChart = await evals.results.generateConfusionMatrixChart('result-123', {
  actual: 'output.category',
  predicted: 'prediction.category',
  normalized: true
})

// Generate a distribution chart
const distributionChart = await evals.results.generateDistributionChart('result-123', {
  field: 'prediction.category',
  type: 'pie'
})

// Generate a metrics over time chart
const metricsOverTimeChart = await evals.results.generateMetricsOverTimeChart({
  target: {
    type: 'function',
    id: 'func-123'
  },
  metrics: ['accuracy', 'f1_score'],
  timeRange: {
    start: '2023-01-01T00:00:00Z',
    end: '2023-06-30T23:59:59Z'
  },
  interval: 'month',
  type: 'line'
})
```

## Result Reporting

Generate reports using the Evals.do API:

```typescript
// Generate a result report
const reportUrl = await evals.results.generateReport('result-123', {
  format: 'pdf',
  includeExamples: true,
  includeMetrics: true,
  includeCharts: true,
  title: 'Evaluation Result Report',
  description: 'A report of evaluation results'
})

// Generate a comparison report
const comparisonReportUrl = await evals.results.generateComparisonReport(['result-123', 'result-456'], {
  format: 'pdf',
  metrics: ['accuracy', 'f1_score', 'precision', 'recall'],
  includeExamples: true,
  includeCharts: true,
  title: 'Evaluation Comparison Report',
  description: 'A comparison of evaluation results'
})

// Schedule a recurring report
await evals.results.scheduleReport({
  name: 'Monthly Evaluation Report',
  description: 'A monthly report of evaluation results',
  target: {
    type: 'function',
    id: 'func-123'
  },
  format: 'pdf',
  includeExamples: true,
  includeMetrics: true,
  includeCharts: true,
  schedule: {
    frequency: 'monthly',
    day: 1,
    time: '00:00:00Z'
  },
  recipients: ['user1@example.com', 'user2@example.com']
})

// Get all scheduled reports
const scheduledReports = await evals.results.listScheduledReports({
  limit: 10,
  offset: 0
})

// Get a specific scheduled report
const scheduledReport = await evals.results.getScheduledReport('report-123')

// Update a scheduled report
const updatedScheduledReport = await evals.results.updateScheduledReport('report-123', {
  name: 'Updated Monthly Evaluation Report',
  description: 'An updated monthly report of evaluation results',
  recipients: ['user1@example.com', 'user2@example.com', 'user3@example.com']
})

// Delete a scheduled report
await evals.results.deleteScheduledReport('report-123')
```

## Result Notifications

Configure result notifications using the Evals.do API:

```typescript
// Configure result notifications
await evals.results.configureNotifications({
  target: {
    type: 'function',
    id: 'func-123'
  },
  conditions: [
    {
      metric: 'accuracy',
      operator: 'lt',
      value: 0.9,
      name: 'Low Accuracy Alert'
    },
    {
      metric: 'error_rate',
      operator: 'gt',
      value: 0.1,
      name: 'High Error Rate Alert'
    }
  ],
  channels: {
    email: ['user1@example.com', 'user2@example.com'],
    slack: ['#alerts']
  }
})

// Get result notification configuration
const notifications = await evals.results.getNotifications({
  target: {
    type: 'function',
    id: 'func-123'
  }
})

// Update result notification configuration
await evals.results.updateNotifications({
  target: {
    type: 'function',
    id: 'func-123'
  },
  conditions: [
    {
      metric: 'accuracy',
      operator: 'lt',
      value: 0.85,
      name: 'Low Accuracy Alert'
    },
    {
      metric: 'error_rate',
      operator: 'gt',
      value: 0.15,
      name: 'High Error Rate Alert'
    }
  ],
  channels: {
    email: ['user1@example.com', 'user2@example.com', 'user3@example.com'],
    slack: ['#alerts', '#monitoring']
  }
})

// Delete result notification configuration
await evals.results.deleteNotifications({
  target: {
    type: 'function',
    id: 'func-123'
  }
})
```

## Result Exports

Export results using the Evals.do API:

```typescript
// Export results
const exportUrl = await evals.results.export('result-123', {
  format: 'json',
  includeExamples: true,
  includeMetrics: true
})

// Export comparison
const comparisonExportUrl = await evals.results.exportComparison(['result-123', 'result-456'], {
  format: 'csv',
  metrics: ['accuracy', 'f1_score', 'precision', 'recall']
})

// Export examples
const examplesExportUrl = await evals.results.exportExamples('result-123', {
  format: 'json',
  limit: 1000
})

// Export metrics
const metricsExportUrl = await evals.results.exportMetrics('result-123', {
  format: 'csv',
  metrics: ['accuracy', 'f1_score', 'precision', 'recall']
})
```

## Result Sharing

Share results using the Evals.do API:

```typescript
// Share a result with a user
await evals.results.share('result-123', {
  user: 'user-123',
  permission: 'read'
})

// Share a result with a team
await evals.results.share('result-123', {
  team: 'team-123',
  permission: 'write'
})

// Share a result with an organization
await evals.results.share('result-123', {
  organization: 'org-123',
  permission: 'admin'
})

// Get result permissions
const permissions = await evals.results.getPermissions('result-123')

// Update result permissions
await evals.results.updatePermissions('result-123', {
  user: 'user-123',
  permission: 'write'
})

// Remove result permissions
await evals.results.removePermissions('result-123', {
  user: 'user-123'
})
```

## Next Steps

- [Analyze your first evaluation results](/evals/results/analysis)
- [Compare model performance](/evals/results/comparison)
- [Create custom visualizations](/evals/results/visualization)