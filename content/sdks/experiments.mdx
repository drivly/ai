---
title: experiments.do
collection: sdks
---

# experiments.do

[![npm version](https://img.shields.io/npm/v/experiments.do.svg)](https://www.npmjs.com/package/experiments.do)
[![npm downloads](https://img.shields.io/npm/dm/experiments.do.svg)](https://www.npmjs.com/package/experiments.do)
[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)
[![TypeScript](https://img.shields.io/badge/TypeScript-4.9.5-blue.svg)](https://www.typescriptlang.org/)
[![Discord](https://img.shields.io/badge/Discord-Join%20Chat-7289da?logo=discord&logoColor=white)](https://discord.gg/tafnNeUQdm)
[![GitHub Issues](https://img.shields.io/github/issues/drivly/ai.svg)](https://github.com/drivly/ai/issues)
[![GitHub Stars](https://img.shields.io/github/stars/drivly/ai.svg)](https://github.com/drivly/ai)

Experiments.do provides a powerful framework for testing hypotheses, measuring outcomes, and iteratively improving your AI applications. It enables you to run controlled experiments to validate ideas, optimize performance, and make data-driven decisions.

## Key Features

- **A/B Testing**: Compare different configurations side by side
- **Metrics Tracking**: Measure and analyze performance metrics
- **Versioning**: Track changes and iterations
- **Reproducibility**: Ensure experiments can be reproduced

## Installation

```bash
npm install experiments.do
# or
yarn add experiments.do
# or
pnpm add experiments.do
```

## Usage

### Defining Experiments

```typescript
import { ExperimentsClient } from 'experiments.do'

const experiments = new ExperimentsClient({
  apiKey: 'your-api-key',
})

// Define an experiment
const experiment = {
  name: 'SummarizationExperiment',
  description: 'Compare different models and prompts for text summarization',
  variants: [
    {
      id: 'baseline',
      description: 'Current production configuration',
      config: {
        model: 'gpt-3.5-turbo',
        prompt: 'summarize-v1',
        temperature: 0.7,
        maxTokens: 150,
      },
    },
    {
      id: 'new-model',
      description: 'Testing newer model',
      config: {
        model: 'gpt-4',
        prompt: 'summarize-v1',
        temperature: 0.7,
        maxTokens: 150,
      },
    },
  ],
  metrics: [
    {
      name: 'rouge-1',
      description: 'ROUGE-1 score against reference summaries',
      higherIsBetter: true,
    },
    {
      name: 'latency',
      description: 'Response time in milliseconds',
      higherIsBetter: false,
    },
  ],
  trafficAllocation: {
    type: 'percentage',
    values: {
      baseline: 50,
      'new-model': 50,
    },
  },
}

// Create the experiment
await experiments.create(experiment)
```

### Running Experiments

```typescript
// Start an experiment
await experiments.start('SummarizationExperiment')

// Get a variant for a specific request
const variant = await experiments.getVariant('SummarizationExperiment', {
  userId: 'user-123',
  sessionId: 'session-456',
})

// Use the variant in your application
const summary = await llm.generate({
  model: variant.config.model,
  prompt: prompts.get(variant.config.prompt),
  temperature: variant.config.temperature,
  maxTokens: variant.config.maxTokens,
  input: {
    text: articleText,
  },
})

// Record metrics for the variant
await experiments.recordMetrics('SummarizationExperiment', variant.id, {
  'rouge-1': 0.78,
  latency: 450,
})
```

### Analyzing Results

```typescript
// Get experiment results
const results = await experiments.getResults('SummarizationExperiment')

// Compare variants
const comparison = await experiments.compareVariants('SummarizationExperiment', ['baseline', 'new-model'])

// Get recommendations
const recommendations = await experiments.getRecommendations('SummarizationExperiment')
```

## API Reference

### `ExperimentsClient`

The main client for interacting with the Experiments.do API.

#### Constructor

```typescript
new ExperimentsClient({
  apiKey?: string,
  baseUrl?: string,
  flagsApiKey?: string,
  flagsBaseUrl?: string,
})
```

#### Methods

- `create(experiment: Experiment): Promise<Experiment>` - Create a new experiment
- `start(experimentName: string): Promise<any>` - Start an experiment
- `getVariant(experimentName: string, context: VariantContext): Promise<VariantResult>` - Get a variant for a specific context
- `recordMetrics(experimentName: string, variantId: string, metrics: Record<string, number>): Promise<any>` - Record metrics for a variant
- `getResults(experimentName: string): Promise<ExperimentResults>` - Get experiment results
- `compareVariants(experimentName: string, variantIds: string[]): Promise<ExperimentComparison>` - Compare variants
- `getRecommendations(experimentName: string): Promise<ExperimentRecommendation>` - Get recommendations
- `list(params?: QueryParams): Promise<ListResponse<Experiment>>` - List experiments
- `get(experimentId: string): Promise<Experiment>` - Get an experiment by ID
- `update(experimentId: string, data: Partial<Experiment>): Promise<Experiment>` - Update an experiment
- `delete(experimentId: string): Promise<any>` - Delete an experiment

## Dependencies

- [apis.do](https://www.npmjs.com/package/apis.do) - Unified API Gateway for all domains and services in the .do ecosystem
