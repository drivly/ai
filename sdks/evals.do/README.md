# [evals.do](https://evals.do)

[![npm version](https://img.shields.io/npm/v/evals.do.svg)](https://www.npmjs.com/package/evals.do)
[![npm downloads](https://img.shields.io/npm/dm/evals.do.svg)](https://www.npmjs.com/package/evals.do)
[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)
[![TypeScript](https://img.shields.io/badge/TypeScript-4.9.5-blue.svg)](https://www.typescriptlang.org/)
[![GitHub Issues](https://img.shields.io/github/issues/drivly/ai.svg)](https://github.com/drivly/ai/issues)

## Measure & Improve AI Component Performance

Evals.do is a comprehensive toolkit for evaluating, measuring, and improving the performance of AI components, including functions, workflows, and agents. It provides a standardized way to assess AI system quality across multiple dimensions such as accuracy, reliability, and efficiency.

## Features

- **Standardized Evaluation Metrics**: Consistent measurement of AI component performance
- **Comparative Analysis**: Benchmark different models, prompts, and configurations
- **Automated Testing**: Streamlined evaluation pipelines for continuous improvement
- **Custom Evaluation Criteria**: Define domain-specific evaluation standards
- **Performance Visualization**: Intuitive dashboards for analyzing evaluation results
- **Integration with Development Workflow**: Seamless connection to your development process
- **Type-Safe Development**: Full TypeScript support for reliable evaluation implementation

## Installation

```bash
npm install evals.do
# or
yarn add evals.do
# or
pnpm add evals.do
```

## Quick Start

```typescript
import { createEval } from 'evals.do'

// Define an evaluation for a summarization function
const summarizationEval = createEval({
  name: 'Article Summarization',
  description: 'Evaluates the quality of article summaries generated by different models',

  // Define the test cases
  testCases: [
    {
      input: { article: 'Long article text...' },
      expectedOutput: {
        summary: 'Example of an ideal summary...',
        keyPoints: ['Key point 1', 'Key point 2', 'Key point 3'],
      },
    },
    // More test cases...
  ],

  // Define evaluation metrics
  metrics: {
    accuracy: {
      description: 'Measures how accurately the summary captures the main points of the article',
      scoreFunction: async (result, expected) => {
        // Implementation of the scoring function
        return { score: 0.85, details: 'Captured 85% of key points' }
      },
    },
    conciseness: {
      description: 'Measures how concise the summary is while retaining important information',
      scoreFunction: async (result, expected) => {
        // Implementation of the scoring function
        return { score: 0.92, details: 'Excellent brevity' }
      },
    },
  },
})

// Run the evaluation against a specific AI function
const results = await summarizationEval.run({
  target: ai.summarizeArticle,
  options: {
    model: 'gpt-4',
    temperature: 0.2,
  },
})

console.log(results)
// {
//   overallScore: 0.88,
//   metrics: {
//     accuracy: { score: 0.85, details: 'Captured 85% of key points' },
//     conciseness: { score: 0.92, details: 'Excellent brevity' }
//   },
//   testCaseResults: [
//     {
//       testCase: { input: {...}, expectedOutput: {...} },
//       output: { summary: '...', keyPoints: [...] },
//       scores: {
//         accuracy: { score: 0.85, details: '...' },
//         conciseness: { score: 0.92, details: '...' }
//       }
//     },
//     // More test case results...
//   ]
// }
```

## Core Concepts

### Evaluations

Evaluations are structured assessments of AI component performance against specific criteria.

```typescript
import { defineEval } from 'evals.do'

const classificationEval = defineEval({
  name: 'Sentiment Classification',
  description: 'Evaluates the accuracy of sentiment classification models',
  testCases: [...],
  metrics: {...}
})
```

### Metrics

Metrics define the specific aspects of performance being measured.

```typescript
const metrics = {
  accuracy: {
    description: 'Percentage of correct classifications',
    scoreFunction: async (result, expected) => {
      const correct = result.label === expected.label
      return { score: correct ? 1.0 : 0.0, details: correct ? 'Correct' : 'Incorrect' }
    },
  },
  confidence: {
    description: 'Confidence level of the classification',
    scoreFunction: async (result, expected) => {
      // Score based on confidence value when correct, penalty when incorrect
      const isCorrect = result.label === expected.label
      const score = isCorrect ? result.confidence : 1 - result.confidence
      return { score, details: `Confidence: ${result.confidence}` }
    },
  },
}
```

### Benchmarks

Compare performance across different models or configurations.

```typescript
import { createBenchmark } from 'evals.do'

const sentimentBenchmark = createBenchmark({
  name: 'Sentiment Analysis Benchmark',
  description: 'Compares different models for sentiment analysis performance',
  evaluation: sentimentEval,

  configurations: [
    { name: 'GPT-4o', options: { model: 'gpt-4o', temperature: 0.2 } },
    { name: 'Claude 3.5 Sonnet', options: { model: 'claude-3-5-sonnet', temperature: 0.2 } },
    { name: 'Mistral Large 2', options: { model: 'mistral-large-2', temperature: 0.2 } },
    { name: 'Gemini 1.5 Pro', options: { model: 'gemini-1.5-pro', temperature: 0.2 } },
  ],
})

const benchmarkResults = await sentimentBenchmark.run()
```

## API Reference

### Core Functions

- `createEval(config)`: Create a new evaluation
- `defineEval(config)`: Alternative way to define an evaluation
- `createBenchmark(config)`: Create a benchmark for comparing multiple configurations
- `createTestSuite(config)`: Group related evaluations into a test suite
- `defineMetric(config)`: Create a reusable evaluation metric

### Evaluation Configuration

- `name`: The name of the evaluation
- `description`: A description of what's being evaluated
- `testCases`: The test cases to evaluate against
- `metrics`: The metrics to measure
- `aggregation`: How to aggregate scores across test cases

### Benchmark Configuration

- `name`: The name of the benchmark
- `description`: A description of the benchmark
- `evaluation`: The evaluation to use for benchmarking
- `configurations`: The different configurations to compare
- `visualizations`: Visualization options for results

## Examples

Check out the [examples directory](https://github.com/drivly/ai/tree/main/examples) for more usage examples.

## Contributing

We welcome contributions! Please see our [Contributing Guide](https://github.com/drivly/ai/blob/main/CONTRIBUTING.md) for more details.

## License

[MIT](https://opensource.org/licenses/MIT)
